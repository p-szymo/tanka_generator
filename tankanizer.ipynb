{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# functions for this project\n",
    "from functions import *\n",
    "from pronouncing import phones_for_word, syllable_count\n",
    "\n",
    "# word list creation options\n",
    "import wordninja\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "\n",
    "# file openers\n",
    "import json\n",
    "from os import path, listdir\n",
    "import gzip\n",
    "import pickle\n",
    "\n",
    "# miscellany\n",
    "import time\n",
    "\n",
    "# reload functions/libraries when edited\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to load\n",
    "with open('data/own_corpus.txt') as hello:\n",
    "    text = hello.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In order for words to be recognized by the ```phones_for_word``` function, you need to get it into proper format by lowercasing, correcting any characters with nonstandard encoding, and removing any hyphens that may create compound words not found supported within the function.\n",
    "\n",
    "*NOTE: Depending on your corpus, more (or less) processing of the text may be required.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"april 17, 2020\\n\\n10 minutes # 13\\n\\n\\n\\nyou trust the limp in your step and fetch a loose post from the earth's grip throw it in the back of the wagon and drive on down to the place you used to call home now known for its unmown lawn its seeding grass propagating the cracks in the driveway the fresh locks shiny next to the grimed siding it seemed the perfect tool to bust in and grab that which remained the one thing you'd forgotten in the one place you hadn't thought to look the false wall beside within the second drawer in the second row the one place you knew you could keep his picture\\n\\n\\n\\nanywayyyy\\n\\n\\n\\nmy's well write a bit of feelings even though i've deadended deadenced deadened myself on borekas and wine really slown down huh i really thought slown was word for a long second guess it in't fluck me i dunno i feel a bit of the weight lifted but still pressed i can't pop right back up again body jam like a stubbed toe and my neckspine has shifted to constant pain the hunch and lean employe\""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lowercase text, correct apostrophe, convert hyphens to spaces\n",
    "text_formatted = text.lower().replace(\"’\", \"'\").replace('-', ' ').replace('—', ' ')\n",
    "text_formatted[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tokenize text, i.e. convert string into a list of words.\n",
    "    - There are several ways to do this, the simplest being ```text.split()```.\n",
    "    - Below is a slightly more sophisticated way that disregards numbers and punctuation; it also keeps words with apostrophes intact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pattern that grabs all words, without splitting them on apostrophes\n",
    "tokenizer = RegexpTokenizer(pattern=\"[a-zA-Z']+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['april',\n",
       " 'minutes',\n",
       " 'you',\n",
       " 'trust',\n",
       " 'the',\n",
       " 'limp',\n",
       " 'in',\n",
       " 'your',\n",
       " 'step',\n",
       " 'and',\n",
       " 'fetch',\n",
       " 'a',\n",
       " 'loose',\n",
       " 'post',\n",
       " 'from',\n",
       " 'the',\n",
       " \"earth's\",\n",
       " 'grip',\n",
       " 'throw',\n",
       " 'it']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a list of words from the corpus\n",
    "corpus_words = tokenizer.tokenize(text_formatted)\n",
    "corpus_words[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Markov dictionary\n",
    "\n",
    "- The ```countable_corpus``` ensures that words which may be chosen from the dictionary have a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a dictionary\n",
    "defaultdict(list)\n",
    "\n",
    "# create Markov dictionary\n",
    "text_dictionary = countable_corpus(corpus_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------\n",
      "\n",
      "     TANKA #1\n",
      "\n",
      "quality of the\n",
      "shadow which in reaping the\n",
      "house for prophets what\n",
      "they were growing worried the\n",
      "super successful me the\n",
      "\n",
      "----------------------------\n",
      "\n",
      "     TANKA #2\n",
      "\n",
      "jester pie of the\n",
      "years older than i right now\n",
      "they shout to be the\n",
      "bleeding she was booze one place\n",
      "on the ambulance is bad\n",
      "\n",
      "----------------------------\n",
      "\n",
      "     TANKA #3\n",
      "\n",
      "du labor on the\n",
      "sixth of time for we can stay\n",
      "et there skin hot sex\n",
      "out by thugs with you can choke\n",
      "on fermented root women\n",
      "\n",
      "----------------------------\n",
      "\n",
      "     TANKA #4\n",
      "\n",
      "radio like the\n",
      "refreshing chill parasites\n",
      "lost forget where is\n",
      "all back giving me in the\n",
      "gate outside and tuck tucker\n",
      "\n",
      "----------------------------\n",
      "\n",
      "     TANKA #5\n",
      "\n",
      "musty swampy pit\n",
      "and grasp how faux how much the\n",
      "bad reflex but dead\n",
      "here it just can't be alive\n",
      "i do indeed i think of\n",
      "\n",
      "----------------------------\n",
      "\n",
      "     TANKA #6\n",
      "\n",
      "woke up by a the\n",
      "towns all may not mention him\n",
      "and me makes the the\n",
      "dull a spectacular for\n",
      "the degree of my response\n",
      "\n",
      "----------------------------\n",
      "\n",
      "     TANKA #7\n",
      "\n",
      "plunged back but i'm now\n",
      "you can't do that could be the\n",
      "most likely set the\n",
      "writer if you can't even\n",
      "thinks of my calm change it for\n",
      "\n",
      "----------------------------\n",
      "\n",
      "     TANKA #8\n",
      "\n",
      "mush grown on a the\n",
      "name benjamin do you this\n",
      "is smudge smudge smudge what\n",
      "i'm racing downhill and my\n",
      "eye i n c t o o\n",
      "\n",
      "----------------------------\n",
      "\n",
      "     TANKA #9\n",
      "\n",
      "pilots incomplete\n",
      "poem into the very\n",
      "smooth on which i the\n",
      "figure it across the the\n",
      "hall when you're staying indoors\n",
      "\n",
      "----------------------------\n",
      "\n",
      "     TANKA #10\n",
      "\n",
      "aggressive lady\n",
      "in at smudge you'll see the oh\n",
      "so easy and i\n",
      "could even faster than to\n",
      "pull sea from his end you too\n",
      "\n",
      "----------------------------\n"
     ]
    }
   ],
   "source": [
    "# print 10 tankas\n",
    "\n",
    "n = 10\n",
    "for i in range(n):\n",
    "    print('\\n----------------------------\\n')\n",
    "    print(f'     TANKA #{i+1}\\n')\n",
    "    print(tankanizer(text_dictionary))\n",
    "    \n",
    "    if i == n-1:\n",
    "        print('\\n----------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For posterity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tub they have to bust\n",
      "out quick succession of the\n",
      "brain juices in their\n",
      "wicked sperm baby jesus\n",
      "cream you because it's new york\n"
     ]
    }
   ],
   "source": [
    "print(tankanizer(text_dictionary))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
